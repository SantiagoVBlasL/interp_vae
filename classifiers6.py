#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
classifiers6.py
===============

M√≥dulo centralizado para definir clasificadores de scikit-learn y sus grids de
b√∫squeda de hiperpar√°metros, encapsulados en un pipeline robusto con pre-procesado.

Versi√≥n: 3.2.0 - Fallback a CPU para RandomForest
Cambios desde v3.1.1:
- Se revierte RandomForest a la implementaci√≥n de scikit-learn (CPU) para
  m√°xima estabilidad y compatibilidad con el pipeline.
- Corregida la l√≥gica de construcci√≥n del pipeline para evitar nombres duplicados.
- Se usa siempre SMOTE de imblearn para evitar errores de compatibilidad con GPU.
"""
from __future__ import annotations
import warnings, logging
warnings.filterwarnings("ignore")
logging.getLogger("lightgbm").setLevel(logging.ERROR)
logging.getLogger("catboost").setLevel(logging.ERROR)
logging.getLogger("sklearn").setLevel(logging.ERROR)

import torch
import numpy as np 
from typing import Any, Dict, List, Tuple
import os

# 1Ô∏è‚É£  Configuraci√≥n del logger principal
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# 2Ô∏è‚É£  Silenciar librer√≠as ruidosas
for noisy in ["lightgbm", "optuna", "sklearn", "xgboost"]:
    logging.getLogger(noisy).setLevel(logging.ERROR)

# Comprobaci√≥n de GPU
try:
    import cupy as cp
    has_gpu = cp.cuda.runtime.getDeviceCount() > 0
except (ImportError, cp.cuda.runtime.CUDARuntimeError):
    has_gpu = False

print("¬øGPU visible?:", has_gpu)

os.environ["XGB_HIDE_LOG"] = "1"

# Dependencias de scikit-learn y otras librer√≠as
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.calibration import CalibratedClassifierCV
from imblearn.pipeline import Pipeline as ImblearnPipeline
from imblearn.over_sampling import SMOTE
from optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution
# justo despu√©s de los otros imports LightGBM
from lightgbm.basic import _LIB, _safe_call   # üëà nuevo


# Definimos un tipo para la salida para mayor claridad
ClassifierPipelineAndGrid = Tuple[ImblearnPipeline, Dict[str, Any], int]

def get_available_classifiers() -> List[str]:
    """Devuelve la lista de clasificadores soportados."""
    return ["rf", "gb", "svm", "logreg", "mlp", "xgb", "cat"]

def _parse_hidden_layers(hidden_layers_str: str | None) -> Tuple[int, ...]:
    """Convierte un string '128,64' a una tupla (128, 64)."""
    if not hidden_layers_str:
        return (128, 64)
    return tuple(int(x.strip()) for x in hidden_layers_str.split(',') if x.strip())

def get_classifier_and_grid(
    classifier_type: str,
    *,
    seed: int = 42,
    balance: bool = False,
    use_smote: bool = False,
    tune_sampler_params: bool = False,
    mlp_hidden_layers: str = "128,64",
    calibrate: bool = False
) -> ClassifierPipelineAndGrid:
    """
    Construye un pipeline de imblearn y devuelve el pipeline, el grid de par√°metros y el n_iter.

    El pipeline contiene:
    1. Un escalador (StandardScaler o 'passthrough').
    2. Un sampler opcional (SMOTE de imblearn).
    3. Un clasificador opcionalmente calibrado.
    """
    ctype = classifier_type.lower()
    if ctype not in get_available_classifiers():
        raise ValueError(f"Tipo de clasificador no soportado: {classifier_type!r}")

    # --- 1. Definir el modelo base y su grid de b√∫squeda ---
    class_weight = "balanced" if balance else None
    model: Any
    param_distributions: Dict[str, Any]
    n_iter_search = 150  # Default n_iter

    # Prefijo 'model__' para los par√°metros del clasificador dentro del pipeline
    if ctype == 'svm':
        model = SVC(probability=True, random_state=seed, class_weight=class_weight, cache_size=500) # cache_size puede ayudar
        param_distributions = {
            # Rango m√°s amplio para C, permitiendo regularizaci√≥n m√°s fuerte o m√°s d√©bil
            'model__C': FloatDistribution(1, 1e4, log=True), 
            # Rango m√°s amplio para gamma, a menudo el valor √≥ptimo es muy peque√±o
            'model__gamma': FloatDistribution(1e-7, 1e-2, log=True),
            'model__kernel': CategoricalDistribution(['rbf']),
        }
        n_iter_search = 160

    elif ctype == 'logreg':
        model = LogisticRegression(random_state=seed, class_weight=class_weight, solver='liblinear', max_iter=2000)
        param_distributions = {
            # El rango actual es bueno, no necesita cambios dr√°sticos
            'model__C': FloatDistribution(1e-5, 1, log=True)
        }
        n_iter_search = 160

    elif ctype == "gb":
        # --- 1. Instancia base ---
        model = LGBMClassifier(
            random_state=seed,
            objective="binary",
            class_weight=class_weight,
            n_jobs=1,          # toda la paralelizaci√≥n la lleva Optuna
            verbose=-1
        )

        # --- 2. Soporte GPU limpio ---
        if has_gpu:
            try:
                if bool(_safe_call(_LIB.LGBM_HasGPU())):
                    model.set_params(device_type="gpu", gpu_use_dp=True)
                    print("[LightGBM] ‚ûú GPU activada")
                else:
                    model.set_params(device_type="cpu")
                    print("[LightGBM] ‚ö† Build sin GPU, usando CPU")
            except Exception:
                model.set_params(device_type="cpu")
                print("[LightGBM] ‚ö† No se pudo comprobar la GPU, usando CPU")

        # --- 3. Espacio de b√∫squeda Optuna ---
        param_distributions = {
            # Profundidad y tama√±o del √°rbol
            "model__max_depth":       IntDistribution(3, 10),
            "model__num_leaves":      IntDistribution(8, 2**10),   # coherente con max_depth

            # Muestras y features por √°rbol
            "model__bagging_fraction":FloatDistribution(0.5, 1.0),
            "model__feature_fraction":FloatDistribution(0.5, 1.0),
            "model__bagging_freq":    IntDistribution(1, 10),

            # Aprendizaje
            "model__learning_rate":   FloatDistribution(5e-4, 0.05, log=True),
            "model__n_estimators":    IntDistribution(300, 2000),

            # Regularizaci√≥n
            "model__min_child_samples":IntDistribution(5, 50),
            "model__min_child_weight": FloatDistribution(1e-3, 100, log=True),
            "model__min_split_gain":   FloatDistribution(0.0, 1.0),
            "model__reg_alpha":        FloatDistribution(1e-3, 10.0, log=True),
            "model__reg_lambda":       FloatDistribution(1e-3, 10.0, log=True),
        }

        # --- 4. N√∫mero de iteraciones din√°mico ---
        n_param       = len(param_distributions)
        n_iter_search = int(round((15 * n_param) / 10.0)) * 10  # m√∫ltiplo de 10


    elif ctype == 'rf':
        print("[RandomForest] ‚ûú Usando implementaci√≥n de scikit-learn (CPU).")
        model = RandomForestClassifier(random_state=seed, class_weight=class_weight, n_jobs=-1)
        param_distributions = {
            'model__n_estimators': IntDistribution(100, 1000),
            'model__max_features': CategoricalDistribution(['sqrt', 'log2', 0.2, 0.4]), # A√±adir proporciones num√©ricas
            # Explorar m√°s granularmente la profundidad
            'model__max_depth': IntDistribution(8, 50),
            'model__min_samples_split': IntDistribution(2, 30), 
            'model__min_samples_leaf': IntDistribution(1, 20)
        }
        n_iter_search = 100
    
    elif ctype == 'mlp':
        hidden = _parse_hidden_layers(mlp_hidden_layers)
        model = MLPClassifier(random_state=seed, hidden_layer_sizes=hidden, max_iter=1000, early_stopping=True, n_iter_no_change=25)
        param_distributions = {
            # Ampliar un poco el rango de regularizaci√≥n
            'model__alpha': FloatDistribution(1e-7, 1e-2, log=True),
            'model__learning_rate_init': FloatDistribution(1e-5, 1e-2, log=True),
        }
        n_iter_search = 120

    elif ctype == "xgb":
        # Sugerido: expl√≠citamente gpu_hist y gpu_id
        model = XGBClassifier(
            random_state=seed,
            eval_metric="auc",
            n_jobs=1,
            tree_method="hist",        # GPU/CPU se decide con 'device'
            device="cuda",           # GPU
            verbosity=0
        )
        if has_gpu:
            print("[XGBoost] ‚ûú  Se usar√° GPU (device=cuda)")
        else:
            print("[XGBoost] ‚ö†  GPU no disponible, usando CPU.")

        param_distributions = {
            "model__gamma": FloatDistribution(0.0, 5.0),
            "model__n_estimators": IntDistribution(150, 1600), # Rango ajustado
            "model__learning_rate": FloatDistribution(1e-5, 0.1, log=True),
            "model__max_depth": IntDistribution(3, 12),
            "model__subsample": FloatDistribution(0.3, 1.1),
            "model__colsample_bytree": FloatDistribution(0.5, 1.1),
            # min_child_weight es un par√°metro de regularizaci√≥n importante
            "model__min_child_weight": FloatDistribution(0.5, 1.1, log=True),
        }
        n_iter_search = 200

    elif ctype == "cat":
        model = CatBoostClassifier(random_state=seed, eval_metric="Logloss", verbose=0, loss_function="Logloss", thread_count=1)
        if has_gpu:
            model.set_params(task_type="GPU", devices="0:0")
            print("[CatBoost] ‚ûú  Se usar√° GPU")
        else:
            print("[CatBoost] ‚ö†  GPU no disponible, usando CPU.")

        param_distributions = {
            "model__depth": IntDistribution(4, 8),
            "model__learning_rate": FloatDistribution(1e-3, 0.08, log=True),
            "model__l2_leaf_reg": FloatDistribution(0.1, 20.0, log=True),
            "model__iterations": IntDistribution(400, 1500),
            "model__bagging_temperature": FloatDistribution(0.1, 0.9),
        }
        n_iter_search = 120


    # --- 2. Opcionalmente, envolver el modelo en un CalibratedClassifierCV ---
    if calibrate and ctype in ["svm", "gb", "rf"]:
        # ... (la l√≥gica de calibraci√≥n no necesita cambios) ...
        model = CalibratedClassifierCV(model, method="isotonic", cv=3)
        _cal = CalibratedClassifierCV(model.model if hasattr(model, "model") else model)
        _inner = "estimator" if "estimator" in _cal.get_params() else "base_estimator"
        param_distributions = { f"model__{_inner}__{k.split('__', 1)[1]}": v for k, v in param_distributions.items() }

    # --- 3. Construir el pipeline (L√ìGICA CORREGIDA Y SIMPLIFICADA) ---
    pipeline_steps = []

    # Paso 1: Escalador
    # Los modelos basados en √°rboles no requieren escalado de caracter√≠sticas
    if ctype in ['rf', 'gb', 'xgb', 'cat']:
        pipeline_steps.append(('scaler', 'passthrough'))
    else:
        pipeline_steps.append(('scaler', StandardScaler()))

    # Paso 2: SMOTE (opcional, siempre desde imblearn para m√°xima compatibilidad)
    if use_smote:
        logger.info(f"[SMOTE] ‚ûú Usando implementaci√≥n de imblearn (CPU) para el clasificador '{ctype}'.")
        pipeline_steps.append(('smote', SMOTE(random_state=seed)))# Usar n_jobs para acelerar

        if tune_sampler_params:
            # k_neighbors debe ser menor que el n√∫mero de muestras en la clase minoritaria.
            # Un rango de 3 a 25 es generalmente seguro y efectivo para tama√±os de dataset moderados.
            param_distributions['smote__k_neighbors'] = IntDistribution(3, 25)
    

    # Paso 3: Modelo
    pipeline_steps.append(('model', model))
    full_pipeline = ImblearnPipeline(steps=pipeline_steps)
    return full_pipeline, param_distributions, n_iter_search